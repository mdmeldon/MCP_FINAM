import asyncio
import datetime
import json

import pendulum
import streamlit as st
from langchain.tools import tool
from langchain_openai import ChatOpenAI
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_community.agent_toolkits.load_tools import load_tools
from langgraph.prebuilt import create_react_agent as lg_create_react_agent
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig
from src.configs import LangchainConfig
from langchain_community.chat_message_histories import (
    StreamlitChatMessageHistory,
)

from src.presentation.langchain.callback import get_streamlit_cb

history = StreamlitChatMessageHistory(key="chat_messages")


@st.cache_resource(show_spinner=False)
def _cached_client(account_id: str, finam_api_token: str):
    with open(".mcp-server-config.json", "r") as file:
        config = json.loads(file.read())

        if config["finam_mcp"]["transport"] == "stdio":
            config["finam_mcp"]["env"] = {
                "FINAM_API_TOKEN": finam_api_token,
                "FINAM_ACCOUNT_ID": account_id,
            }
        return MultiServerMCPClient(config)


@st.cache_resource(show_spinner=False)
def _cached_llm(cfg: LangchainConfig):
    return ChatOpenAI(
        api_key=cfg.API_KEY,
        base_url=cfg.BASE_URL,
        model=cfg.MODEL,
        streaming=True,
    )


async def _init_agent_async(cfg: LangchainConfig, account_id: str, finam_api_token: str):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LangGraph-–∞–≥–µ–Ω—Ç –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ (–≤ —Ç.—á. MCP)."""
    llm = _cached_llm(cfg)

    @tool
    def get_current_time(tz: str = "UTC") -> pendulum.DateTime:
        """–í–ê–ñ–ù–û! –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å–≤—è–∑–∞–Ω —Å –¥–∞—Ç–æ–π/–≤—Ä–µ–º–µ–Ω–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è!"""
        return pendulum.now(tz)

    tools = load_tools(["ddg-search"])
    tools.append(get_current_time)

    client = _cached_client(account_id=account_id, finam_api_token=finam_api_token)

    try:
        tools_from_mcp = await client.get_tools()
        tools.extend(tools_from_mcp)
    except Exception as e:
        st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å MCP-—Ç—É–ª—ã: {e}")

    agent_graph = lg_create_react_agent(llm, tools)
    return agent_graph


async def _handle_chat_async(agent_graph, messages, callbacks: list, recursion_limit: int = 25):
    config: RunnableConfig = {"callbacks": callbacks, "recursion_limit": recursion_limit}
    return await agent_graph.ainvoke({"messages": messages}, config)


async def _handle_chat_async_stream(agent_graph, messages, callbacks: list, recursion_limit: int = 25) -> str:
    config: RunnableConfig = {"callbacks": callbacks, "recursion_limit": recursion_limit}
    async for _ in agent_graph.astream({"messages": messages}, config):
        pass
    cb = callbacks[0]
    return getattr(cb, "_answer_accum", "")


def _get_persistent_loop() -> asyncio.AbstractEventLoop:
    loop = st.session_state.get("_persistent_loop")
    if loop is None or loop.is_closed():
        loop = asyncio.new_event_loop()
        st.session_state["_persistent_loop"] = loop
    return loop


def create_langchain_app(cfg: LangchainConfig):
    with st.sidebar:
        st.title("Finam AI Assistant")
        st.info(f"–ú–æ–¥–µ–ª—å: {cfg.MODEL}")
        with st.expander("üîë –î–æ—Å—Ç—É–ø—ã:"):
            finam_api_token = st.text_input("Finam API —Ç–æ–∫–µ–Ω:", value="", help="–û—Å—Ç–∞–≤—å—Ç–µ –ø—É—Å—Ç—ã–º –µ—Å–ª–∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
            account_id = st.text_input("ID —Å—á–µ—Ç–∞:", value="", help="–û—Å—Ç–∞–≤—å—Ç–µ –ø—É—Å—Ç—ã–º –µ—Å–ª–∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")

    if not finam_api_token:
        st.sidebar.warning(
            "‚ö†Ô∏è Finam API —Ç–æ–∫–µ–Ω –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Finam API token –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–º –ø–æ–ª–µ."
        )
    else:
        st.sidebar.success("‚úÖ Finam API —Ç–æ–∫–µ–Ω —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

    loop = _get_persistent_loop()
    agent_graph = loop.run_until_complete(_init_agent_async(cfg, account_id, finam_api_token))

    if "messages" not in st.session_state:
        st.session_state["messages"] = [AIMessage(content="–ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?")]

    for msg in st.session_state.messages:
        if type(msg) == AIMessage:
            st.chat_message("assistant").write(msg.content)
        if type(msg) == HumanMessage:
            st.chat_message("user").write(msg.content)

    if prompt := st.chat_input():
        st.session_state.messages.append(HumanMessage(content=prompt))
        st.chat_message("user").write(prompt)

        with st.chat_message("assistant"):
            st_callback = get_streamlit_cb(st.empty(), expand_steps=True)
            last_msg = loop.run_until_complete(
                _handle_chat_async_stream(agent_graph, st.session_state.messages, [st_callback])
            )
            if last_msg:
                st.session_state.messages.append(AIMessage(content=last_msg))